{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import time \n",
    "import tqdm\n",
    "from functools import partial\n",
    "train_on_gpu = True\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['samplesubmission.csv', 'test.csv', 'train.csv']"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'files'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>playtime_forever</th>\n",
       "      <th>is_free</th>\n",
       "      <th>price</th>\n",
       "      <th>genres</th>\n",
       "      <th>categories</th>\n",
       "      <th>tags</th>\n",
       "      <th>purchase_date</th>\n",
       "      <th>release_date</th>\n",
       "      <th>total_positive_reviews</th>\n",
       "      <th>total_negative_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>Adventure,Casual,Indie</td>\n",
       "      <td>Single-player,Steam Trading Cards,Steam Cloud</td>\n",
       "      <td>Indie,Adventure,Story Rich,Casual,Atmospheric,...</td>\n",
       "      <td>Jul 2, 2018</td>\n",
       "      <td>10 Dec, 2013</td>\n",
       "      <td>372.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RPG</td>\n",
       "      <td>Single-player,Partial Controller Support</td>\n",
       "      <td>Mod,Utilities,RPG,Game Development,Singleplaye...</td>\n",
       "      <td>Nov 26, 2016</td>\n",
       "      <td>12 Aug, 2015</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>Adventure,Casual,Indie</td>\n",
       "      <td>Single-player,Full controller support,Steam Tr...</td>\n",
       "      <td>Point &amp; Click,Adventure,Story Rich,Comedy,Indi...</td>\n",
       "      <td>Jul 2, 2018</td>\n",
       "      <td>28 Jan, 2014</td>\n",
       "      <td>3018.0</td>\n",
       "      <td>663.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>False</td>\n",
       "      <td>9900.0</td>\n",
       "      <td>Action,RPG</td>\n",
       "      <td>Single-player,Multi-player,Steam Achievements,...</td>\n",
       "      <td>Medieval,RPG,Open World,Strategy,Sandbox,Actio...</td>\n",
       "      <td>Nov 28, 2016</td>\n",
       "      <td>31 Mar, 2010</td>\n",
       "      <td>63078.0</td>\n",
       "      <td>1746.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>Action,Indie,Strategy</td>\n",
       "      <td>Single-player,Co-op,Steam Achievements,Full co...</td>\n",
       "      <td>Tower Defense,Co-op,Action,Strategy,Online Co-...</td>\n",
       "      <td>Mar 4, 2018</td>\n",
       "      <td>30 Jul, 2012</td>\n",
       "      <td>8841.0</td>\n",
       "      <td>523.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  playtime_forever  is_free   price                  genres  \\\n",
       "0   0          0.000000    False  3700.0  Adventure,Casual,Indie   \n",
       "1   1          0.016667     True     0.0                     RPG   \n",
       "2   2          0.000000    False  5000.0  Adventure,Casual,Indie   \n",
       "3   3          1.533333    False  9900.0              Action,RPG   \n",
       "4   4         22.333333    False  4800.0   Action,Indie,Strategy   \n",
       "\n",
       "                                          categories  \\\n",
       "0      Single-player,Steam Trading Cards,Steam Cloud   \n",
       "1           Single-player,Partial Controller Support   \n",
       "2  Single-player,Full controller support,Steam Tr...   \n",
       "3  Single-player,Multi-player,Steam Achievements,...   \n",
       "4  Single-player,Co-op,Steam Achievements,Full co...   \n",
       "\n",
       "                                                tags purchase_date  \\\n",
       "0  Indie,Adventure,Story Rich,Casual,Atmospheric,...   Jul 2, 2018   \n",
       "1  Mod,Utilities,RPG,Game Development,Singleplaye...  Nov 26, 2016   \n",
       "2  Point & Click,Adventure,Story Rich,Comedy,Indi...   Jul 2, 2018   \n",
       "3  Medieval,RPG,Open World,Strategy,Sandbox,Actio...  Nov 28, 2016   \n",
       "4  Tower Defense,Co-op,Action,Strategy,Online Co-...   Mar 4, 2018   \n",
       "\n",
       "   release_date  total_positive_reviews  total_negative_reviews  \n",
       "0  10 Dec, 2013                   372.0                    96.0  \n",
       "1  12 Aug, 2015                    23.0                     0.0  \n",
       "2  28 Jan, 2014                  3018.0                   663.0  \n",
       "3  31 Mar, 2010                 63078.0                  1746.0  \n",
       "4  30 Jul, 2012                  8841.0                   523.0  "
      ]
     },
     "execution_count": 1054,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(f'{path}/train.csv')\n",
    "test = pd.read_csv(f'{path}/test.csv')\n",
    "sub = pd.read_csv(f'{path}/samplesubmission.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 357 records in train dataset\n",
      "There are 90 records in test dataset\n"
     ]
    }
   ],
   "source": [
    "n_train = len(train)\n",
    "n_test = len(test)\n",
    "print(f'There are {n_train} records in train dataset')\n",
    "print(f'There are {n_test} records in test dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data (Apply to both Train and Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_col_to_dict(l):\n",
    "    d = {}\n",
    "    for s in l:\n",
    "        vl = s.split(',')\n",
    "        for v in vl:\n",
    "            if v in d.keys():\n",
    "                d[v] += 1\n",
    "            else:\n",
    "                d[v] = 1\n",
    "    return d\n",
    "category_d = divide_col_to_dict(train.categories)\n",
    "genre_d = divide_col_to_dict(train.genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit-Transform Train\n",
    "s = train.pop('categories').str.split(',')\n",
    "s_test = test.pop('categories').str.split(',')\n",
    "\n",
    "df1 = pd.DataFrame(mlb.fit_transform(s),columns=mlb.classes_).add_prefix('Category-')\n",
    "df1_test = pd.DataFrame(mlb.transform(s_test),columns=mlb.classes_).add_prefix('Category-')\n",
    "\n",
    "train = train.join(df1)\n",
    "test = test.join(df1_test)\n",
    "\n",
    "\n",
    "s = train.pop('genres').str.split(',')\n",
    "s_test = test.pop('genres').str.split(',')\n",
    "\n",
    "df2 = pd.DataFrame(mlb.fit_transform(s),columns=mlb.classes_).add_prefix('Genre-')\n",
    "df2_test = pd.DataFrame(mlb.transform(s_test),columns=mlb.classes_).add_prefix('Genre-')\n",
    "\n",
    "train = train.join(df2)\n",
    "test = test.join(df2_test)\n",
    "\n",
    "s = train.pop('tags').str.split(',')\n",
    "s_test = test.pop('tags').str.split(',')\n",
    "\n",
    "df3 = pd.DataFrame(mlb.fit_transform(s_test),columns=mlb.classes_).add_prefix('Tag-')\n",
    "df3_test = pd.DataFrame(mlb.transform(s_test),columns=mlb.classes_).add_prefix('Tag-')\n",
    "\n",
    "train = train.join(df3)\n",
    "test = test.join(df3_test)\n",
    "\n",
    "# # Transform Test\n",
    "# s = test.pop('categories').str.split(',')\n",
    "# df1 = pd.DataFrame(mlb.transform(s),columns=mlb.classes_).add_prefix('Category-')\n",
    "# test = test.join(df1)\n",
    "# s = test.pop('genres').str.split(',')\n",
    "# df2 = pd.DataFrame(mlb.transform(s),columns=mlb.classes_).add_prefix('Genre-')\n",
    "# test = test.join(df2)\n",
    "# s = test.pop('tags').str.split(',')\n",
    "# df3 = pd.DataFrame(mlb.transform(s),columns=mlb.classes_).add_prefix('Tag-')\n",
    "# test = test.join(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df_x):\n",
    "    \n",
    "    # Remove ID - Not Useful\n",
    "    df_x.pop('id')\n",
    "    # Change is_free to numeric\n",
    "    df_x.is_free = df_x.is_free.astype(int)\n",
    "    \n",
    "    # Replace NaNs with default values for each Column\n",
    "    df_x['purchase_date'] = df_x['purchase_date'].fillna('Jan 1, 2019')\n",
    "    df_x['release_date'] = df_x['release_date'].fillna('Jan 1, 2018')\n",
    "    df_x['total_positive_reviews'] = df_x['total_positive_reviews'].fillna('0')\n",
    "    df_x['total_negative_reviews'] = df_x['total_negative_reviews'].fillna('0')\n",
    "    \n",
    "    df_x['purchase_date'] = df_x['purchase_date'].str[-2:].astype(int)\n",
    "    df_x['release_date'] = df_x['release_date'].str[-2:].astype(int)\n",
    "    df_x['wait_till_purchase'] = df_x['purchase_date'] - df_x['release_date']\n",
    "    df_x['far_from_2019'] = 19 - df_x['purchase_date']\n",
    "    df_x['review_diff'] = df_x['total_positive_reviews'].astype(int) - df_x['total_negative_reviews'].astype(int)\n",
    "    \n",
    "    df_x = df_x.fillna(0)\n",
    "    \n",
    "    df_x.drop(columns=['purchase_date', 'release_date'])\n",
    "    return df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_train = preprocess(train)\n",
    "pp_test = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_without_y = pp_train.loc[:, pp_train.columns != 'playtime_forever']\n",
    "x = x_without_y.values # Returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "train_normalized = pd.DataFrame(x_scaled, columns=x_without_y.columns)\n",
    "\n",
    "x_test = pp_test.values\n",
    "x_test_scaled = min_max_scaler.transform(x_test) # Do NOT \"FIT\"\n",
    "test_normalized = pd.DataFrame(x_test_scaled, columns=pp_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = preprocessing.MinMaxScaler()\n",
    "y = pp_train.loc[:, pp_train.columns == 'playtime_forever']\n",
    "y_scaled = y_scaler.fit_transform(y)\n",
    "y_normalized = pd.DataFrame(y_scaled, columns=y.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normalized = pd.concat([y_normalized, train_normalized], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playtime_forever</th>\n",
       "      <th>is_free</th>\n",
       "      <th>price</th>\n",
       "      <th>purchase_date</th>\n",
       "      <th>release_date</th>\n",
       "      <th>total_positive_reviews</th>\n",
       "      <th>total_negative_reviews</th>\n",
       "      <th>Category-Captions available</th>\n",
       "      <th>Category-Co-op</th>\n",
       "      <th>Category-Commentary available</th>\n",
       "      <th>...</th>\n",
       "      <th>Tag-War</th>\n",
       "      <th>Tag-Warhammer 40K</th>\n",
       "      <th>Tag-Western</th>\n",
       "      <th>Tag-World War I</th>\n",
       "      <th>Tag-World War II</th>\n",
       "      <th>Tag-Zombies</th>\n",
       "      <th>Tag-eSports</th>\n",
       "      <th>wait_till_purchase</th>\n",
       "      <th>far_from_2019</th>\n",
       "      <th>review_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.004189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000146</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.003370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.010920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.143066</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.201853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.196251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.020052</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.030225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0.006883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.003487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.043112</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.049134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.014238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.005105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.005928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357 rows Ã— 288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     playtime_forever  is_free     price  purchase_date  release_date  \\\n",
       "0            0.000000      0.0  0.000231           0.75      0.538462   \n",
       "1            0.000146      1.0  0.000000           0.25      0.692308   \n",
       "2            0.000000      0.0  0.000313           0.75      0.615385   \n",
       "3            0.013474      0.0  0.000619           0.25      0.307692   \n",
       "4            0.196251      0.0  0.000300           0.75      0.461538   \n",
       "..                ...      ...       ...            ...           ...   \n",
       "352          0.006883      0.0  0.000550           0.50      0.769231   \n",
       "353          0.000000      0.0  0.000425           0.75      0.692308   \n",
       "354          0.000000      0.0  0.000519           0.75      0.692308   \n",
       "355          0.000000      0.0  0.000425           0.50      0.769231   \n",
       "356          0.000000      0.0  0.000625           0.75      0.538462   \n",
       "\n",
       "     total_positive_reviews  total_negative_reviews  \\\n",
       "0                  0.000844                0.000220   \n",
       "1                  0.000052                0.000000   \n",
       "2                  0.006845                0.001520   \n",
       "3                  0.143066                0.004004   \n",
       "4                  0.020052                0.001199   \n",
       "..                      ...                     ...   \n",
       "352                0.000340                0.000209   \n",
       "353                0.043112                0.011120   \n",
       "354                0.011565                0.003942   \n",
       "355                0.001628                0.000365   \n",
       "356                0.002075                0.000234   \n",
       "\n",
       "     Category-Captions available  Category-Co-op  \\\n",
       "0                            0.0             0.0   \n",
       "1                            0.0             0.0   \n",
       "2                            0.0             0.0   \n",
       "3                            0.0             0.0   \n",
       "4                            0.0             1.0   \n",
       "..                           ...             ...   \n",
       "352                          0.0             0.0   \n",
       "353                          0.0             0.0   \n",
       "354                          0.0             0.0   \n",
       "355                          0.0             0.0   \n",
       "356                          0.0             1.0   \n",
       "\n",
       "     Category-Commentary available  ...  Tag-War  Tag-Warhammer 40K  \\\n",
       "0                              0.0  ...      0.0                0.0   \n",
       "1                              0.0  ...      0.0                0.0   \n",
       "2                              0.0  ...      0.0                0.0   \n",
       "3                              0.0  ...      0.0                0.0   \n",
       "4                              0.0  ...      0.0                0.0   \n",
       "..                             ...  ...      ...                ...   \n",
       "352                            0.0  ...      0.0                0.0   \n",
       "353                            0.0  ...      0.0                0.0   \n",
       "354                            0.0  ...      0.0                0.0   \n",
       "355                            0.0  ...      0.0                0.0   \n",
       "356                            0.0  ...      0.0                0.0   \n",
       "\n",
       "     Tag-Western  Tag-World War I  Tag-World War II  Tag-Zombies  Tag-eSports  \\\n",
       "0            0.0              0.0               0.0          0.0          0.0   \n",
       "1            0.0              0.0               0.0          0.0          0.0   \n",
       "2            0.0              0.0               0.0          0.0          0.0   \n",
       "3            0.0              0.0               0.0          0.0          0.0   \n",
       "4            0.0              0.0               0.0          0.0          0.0   \n",
       "..           ...              ...               ...          ...          ...   \n",
       "352          0.0              0.0               0.0          0.0          0.0   \n",
       "353          0.0              0.0               0.0          0.0          0.0   \n",
       "354          0.0              0.0               0.0          0.0          0.0   \n",
       "355          0.0              0.0               0.0          0.0          0.0   \n",
       "356          0.0              0.0               0.0          0.0          0.0   \n",
       "\n",
       "     wait_till_purchase  far_from_2019  review_diff  \n",
       "0              0.500000           0.25     0.004189  \n",
       "1              0.214286           0.75     0.003370  \n",
       "2              0.428571           0.25     0.010920  \n",
       "3              0.571429           0.75     0.201853  \n",
       "4              0.571429           0.25     0.030225  \n",
       "..                  ...            ...          ...  \n",
       "352            0.214286           0.50     0.003487  \n",
       "353            0.357143           0.25     0.049134  \n",
       "354            0.357143           0.25     0.014238  \n",
       "355            0.214286           0.50     0.005105  \n",
       "356            0.500000           0.25     0.005928  \n",
       "\n",
       "[357 rows x 288 columns]"
      ]
     },
     "execution_count": 1063,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class customData(Dataset):\n",
    "    def __init__(self, data):\n",
    "            self.data = torch.FloatTensor(data.values.astype('float'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "            target = self.data[index][0]\n",
    "            data_val = self.data[index] [1:]\n",
    "            return data_val,target\n",
    "train_dataset = customData(train_normalized)\n",
    "# test_dataset = customData(test_normalized)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=Test_Batch_Size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw = train_normalized.pop('playtime_forever')\n",
    "y = Variable(torch.tensor(y.values))\n",
    "x = Variable(torch.tensor(train_normalized.values))\n",
    "x_sub = Variable(torch.tensor(test_normalized.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 1065,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 1066,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 288)"
      ]
     },
     "execution_count": 1067,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.010250518253436671\n",
      "Training loss: 0.009750240263772796\n",
      "Training loss: 0.009741475006796846\n",
      "Training loss: 0.009740915809117331\n",
      "Training loss: 0.00974081331424289\n",
      "Training loss: 0.0097407444445972\n",
      "Training loss: 0.009740679778236004\n",
      "Training loss: 0.009740615258497218\n",
      "Training loss: 0.009740550815191214\n",
      "Training loss: 0.009740486591317262\n",
      "Training loss: 0.009740423219107864\n",
      "Training loss: 0.009740360826236954\n",
      "Training loss: 0.009740299351332348\n",
      "Training loss: 0.00974023749115135\n",
      "Training loss: 0.009740175450063084\n",
      "Training loss: 0.009740114284507682\n",
      "Training loss: 0.009740052903108701\n",
      "Training loss: 0.009739992430842406\n",
      "Training loss: 0.009739932489410309\n",
      "Training loss: 0.009739871527524166\n",
      "Training loss: 0.009739810902290027\n",
      "Training loss: 0.009739750677753312\n",
      "Training loss: 0.009739691295199982\n",
      "Training loss: 0.00973963336927842\n",
      "Training loss: 0.009739574562934481\n",
      "Training loss: 0.009739515850855382\n",
      "Training loss: 0.009739456789782469\n",
      "Training loss: 0.009739398185472706\n",
      "Training loss: 0.00973933884819393\n",
      "Training loss: 0.00973927975402867\n",
      "Training loss: 0.00973922053608449\n",
      "Training loss: 0.009739161871883485\n",
      "Training loss: 0.009739104219229154\n",
      "Training loss: 0.009739047796038616\n",
      "Training loss: 0.009738990080028747\n",
      "Training loss: 0.00973893309991062\n",
      "Training loss: 0.009738875600331855\n",
      "Training loss: 0.009738817797822631\n",
      "Training loss: 0.009738760001236593\n",
      "Training loss: 0.009738701617132788\n",
      "Training loss: 0.009738642915032332\n",
      "Training loss: 0.009738583646274735\n",
      "Training loss: 0.009738523039709525\n",
      "Training loss: 0.009738461301915767\n",
      "Training loss: 0.009738407580256467\n",
      "Training loss: 0.009738354561646626\n",
      "Training loss: 0.00973830213677539\n",
      "Training loss: 0.009738249271634634\n",
      "Training loss: 0.009738196086710199\n",
      "Training loss: 0.0097381440115989\n",
      "Training loss: 0.009738091632798866\n",
      "Training loss: 0.009738038841684721\n",
      "Training loss: 0.009737986034511667\n",
      "Training loss: 0.009737933747414605\n",
      "Training loss: 0.009737881560483306\n",
      "Training loss: 0.00973782944217115\n",
      "Training loss: 0.009737776952063508\n",
      "Training loss: 0.009737724227952852\n",
      "Training loss: 0.00973767199485984\n",
      "Training loss: 0.009737619981998819\n",
      "Training loss: 0.009737567610391245\n",
      "Training loss: 0.009737515155084659\n",
      "Training loss: 0.009737463270269081\n",
      "Training loss: 0.009737411125765948\n",
      "Training loss: 0.009737358658052742\n",
      "Training loss: 0.009737306595058613\n",
      "Training loss: 0.009737254270363996\n",
      "Training loss: 0.009737201729793315\n",
      "Training loss: 0.009737149213268388\n",
      "Training loss: 0.009737096596244171\n",
      "Training loss: 0.009737043964644913\n",
      "Training loss: 0.009736992425428638\n",
      "Training loss: 0.009736940506211428\n",
      "Training loss: 0.009736886869351029\n",
      "Training loss: 0.009736833612928781\n",
      "Training loss: 0.009736779173574723\n",
      "Training loss: 0.009736724812859372\n",
      "Training loss: 0.009736670286720647\n",
      "Training loss: 0.009736616059155401\n",
      "Training loss: 0.009736561877699227\n",
      "Training loss: 0.009736506799867718\n",
      "Training loss: 0.00973645112136002\n",
      "Training loss: 0.00973639342738685\n",
      "Training loss: 0.009736336319113501\n",
      "Training loss: 0.009736278227049866\n",
      "Training loss: 0.009736220059721383\n",
      "Training loss: 0.009736162233322965\n",
      "Training loss: 0.009736105109559879\n",
      "Training loss: 0.009736046648308011\n",
      "Training loss: 0.009735990241706655\n",
      "Training loss: 0.009735933793180229\n",
      "Training loss: 0.009735881667879621\n",
      "Training loss: 0.00973582994737927\n",
      "Training loss: 0.00973577732311414\n",
      "Training loss: 0.009735725528865442\n",
      "Training loss: 0.009735674157049864\n",
      "Training loss: 0.009735621994021098\n",
      "Training loss: 0.009735574222237512\n",
      "Training loss: 0.009735524127515842\n",
      "Training loss: 0.009735475016032053\n",
      "Training loss: 0.009735425186207174\n",
      "Training loss: 0.009735376591788912\n",
      "Training loss: 0.009735326504918256\n",
      "Training loss: 0.009735277890891915\n",
      "Training loss: 0.009735228459531398\n",
      "Training loss: 0.009735179117562906\n",
      "Training loss: 0.009735130986143528\n",
      "Training loss: 0.009735080785248871\n",
      "Training loss: 0.009735032161400363\n",
      "Training loss: 0.009734982440541478\n",
      "Training loss: 0.009734933975632904\n",
      "Training loss: 0.009734884610946773\n",
      "Training loss: 0.00973483666964171\n",
      "Training loss: 0.009734788434184547\n",
      "Training loss: 0.009734739625592119\n",
      "Training loss: 0.00973469228196626\n",
      "Training loss: 0.009734643218348528\n",
      "Training loss: 0.009734594430945511\n",
      "Training loss: 0.009734545235247924\n",
      "Training loss: 0.009734497022640079\n",
      "Training loss: 0.009734447109748542\n",
      "Training loss: 0.009734398173782249\n",
      "Training loss: 0.009734348315947996\n",
      "Training loss: 0.009734299814181335\n",
      "Training loss: 0.009734250029464271\n",
      "Training loss: 0.009734200099155868\n",
      "Training loss: 0.00973415016289521\n",
      "Training loss: 0.009734100053034262\n",
      "Training loss: 0.009734051287947661\n",
      "Training loss: 0.009734000466558439\n",
      "Training loss: 0.00973395017900245\n",
      "Training loss: 0.009733900628899556\n",
      "Training loss: 0.00973385032115949\n",
      "Training loss: 0.009733799337791935\n",
      "Training loss: 0.009733748125968335\n",
      "Training loss: 0.009733699483858967\n",
      "Training loss: 0.00973364862054821\n",
      "Training loss: 0.009733598556426163\n",
      "Training loss: 0.00973354866267979\n",
      "Training loss: 0.009733498231159031\n",
      "Training loss: 0.009733446476769462\n",
      "Training loss: 0.009733396046571833\n",
      "Training loss: 0.009733342446810482\n",
      "Training loss: 0.009733289758404295\n",
      "Training loss: 0.009733236141237226\n",
      "Training loss: 0.009733181618046082\n",
      "Training loss: 0.00973312776418177\n",
      "Training loss: 0.009733074248192069\n",
      "Training loss: 0.00973302170071908\n",
      "Training loss: 0.009732969639003934\n",
      "Training loss: 0.009732917168564397\n",
      "Training loss: 0.009732866407616494\n",
      "Training loss: 0.009732814886267137\n",
      "Training loss: 0.009732763768698173\n",
      "Training loss: 0.009732713195546744\n",
      "Training loss: 0.009732660141871493\n",
      "Training loss: 0.009732608757418224\n",
      "Training loss: 0.009732556740395333\n",
      "Training loss: 0.009732504141170887\n",
      "Training loss: 0.009732453296547424\n",
      "Training loss: 0.009732399521100304\n",
      "Training loss: 0.009732348389026452\n",
      "Training loss: 0.00973229394157739\n",
      "Training loss: 0.00973224080569906\n",
      "Training loss: 0.009732186520927886\n",
      "Training loss: 0.00973213309621259\n",
      "Training loss: 0.009732078386084678\n",
      "Training loss: 0.009732023514337445\n",
      "Training loss: 0.009731970325411187\n",
      "Training loss: 0.009731914908454601\n",
      "Training loss: 0.009731860910627404\n",
      "Training loss: 0.009731805070009423\n",
      "Training loss: 0.009731750388079333\n",
      "Training loss: 0.009731693513332366\n",
      "Training loss: 0.009731637484333266\n",
      "Training loss: 0.009731582609849316\n",
      "Training loss: 0.009731524087487628\n",
      "Training loss: 0.0097314690229567\n",
      "Training loss: 0.009731410808170027\n",
      "Training loss: 0.009731355746374862\n",
      "Training loss: 0.009731300356311236\n",
      "Training loss: 0.009731245056222243\n",
      "Training loss: 0.00973119210830645\n",
      "Training loss: 0.009731135799331162\n",
      "Training loss: 0.009731082771616365\n",
      "Training loss: 0.009731026744997241\n",
      "Training loss: 0.00973097184902263\n",
      "Training loss: 0.009730916117399107\n",
      "Training loss: 0.009730859869132767\n",
      "Training loss: 0.00973080409684698\n",
      "Training loss: 0.00973074663809229\n",
      "Training loss: 0.009730690365818187\n",
      "Training loss: 0.0097306356198518\n",
      "Training loss: 0.009730584764500302\n",
      "Training loss: 0.009730534519338164\n",
      "Training loss: 0.009730483905840248\n",
      "Training loss: 0.009730435086704807\n",
      "Training loss: 0.009730385474765945\n",
      "Training loss: 0.009730335882032334\n",
      "Training loss: 0.009730287687476323\n",
      "Training loss: 0.009730237875503114\n",
      "Training loss: 0.009730188287693042\n",
      "Training loss: 0.009730139698670703\n",
      "Training loss: 0.009730089694208166\n",
      "Training loss: 0.009730039163053782\n",
      "Training loss: 0.009729991042891444\n",
      "Training loss: 0.009729939990825063\n",
      "Training loss: 0.009729889802845771\n",
      "Training loss: 0.009729838651892076\n",
      "Training loss: 0.009729786885533928\n",
      "Training loss: 0.009729736169532006\n",
      "Training loss: 0.009729683442324015\n",
      "Training loss: 0.009729631625619616\n",
      "Training loss: 0.009729581212496438\n",
      "Training loss: 0.009729528778392197\n",
      "Training loss: 0.009729478129856278\n",
      "Training loss: 0.009729426757346055\n",
      "Training loss: 0.009729375082959808\n",
      "Training loss: 0.009729324290110695\n",
      "Training loss: 0.00972927232433877\n",
      "Training loss: 0.009729220317459623\n",
      "Training loss: 0.009729170086122231\n",
      "Training loss: 0.009729117670218088\n",
      "Training loss: 0.009729066087404521\n",
      "Training loss: 0.009729014573097874\n",
      "Training loss: 0.009728962694125516\n",
      "Training loss: 0.009728911099766643\n",
      "Training loss: 0.009728858739427287\n",
      "Training loss: 0.009728806084279484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.009728754672257927\n",
      "Training loss: 0.009728701934372858\n",
      "Training loss: 0.009728649482650101\n",
      "Training loss: 0.009728598206099037\n",
      "Training loss: 0.009728545011331748\n",
      "Training loss: 0.00972849227172605\n",
      "Training loss: 0.00972844067481077\n",
      "Training loss: 0.009728386747732885\n",
      "Training loss: 0.009728334856843556\n",
      "Training loss: 0.009728282202702998\n",
      "Training loss: 0.00972822887166091\n",
      "Training loss: 0.009728177093558672\n",
      "Training loss: 0.009728123035994978\n",
      "Training loss: 0.00972807029758842\n",
      "Training loss: 0.009728017680976728\n",
      "Training loss: 0.009727963774813685\n",
      "Training loss: 0.009727910534770269\n",
      "Training loss: 0.009727858157786876\n",
      "Training loss: 0.009727803329219579\n",
      "Training loss: 0.009727749943095972\n",
      "Training loss: 0.009727695774452589\n",
      "Training loss: 0.009727641905844625\n",
      "Training loss: 0.009727588168222572\n",
      "Training loss: 0.00972753349266066\n",
      "Training loss: 0.009727478805263128\n",
      "Training loss: 0.009727425141812527\n",
      "Training loss: 0.009727370014416281\n",
      "Training loss: 0.00972731523501574\n",
      "Training loss: 0.00972726138709255\n",
      "Training loss: 0.009727204932471653\n",
      "Training loss: 0.009727151024022223\n",
      "Training loss: 0.009727095210392254\n",
      "Training loss: 0.009727040009503488\n",
      "Training loss: 0.009726984953327002\n",
      "Training loss: 0.009726928788671464\n",
      "Training loss: 0.009726873106156836\n",
      "Training loss: 0.009726818184747322\n",
      "Training loss: 0.009726761081413257\n",
      "Training loss: 0.00972670474171159\n",
      "Training loss: 0.009726649817514373\n",
      "Training loss: 0.009726592387855748\n",
      "Training loss: 0.009726536320322055\n",
      "Training loss: 0.00972647952353878\n",
      "Training loss: 0.009726422714879805\n",
      "Training loss: 0.009726366445351414\n",
      "Training loss: 0.009726308996954092\n",
      "Training loss: 0.009726251462475308\n",
      "Training loss: 0.009726198556386392\n",
      "Training loss: 0.009726139169756499\n",
      "Training loss: 0.009726081049816334\n",
      "Training loss: 0.009726027967010515\n",
      "Training loss: 0.009725968501313192\n",
      "Training loss: 0.009725910952328122\n",
      "Training loss: 0.009725853487486924\n",
      "Training loss: 0.009725798068923975\n",
      "Training loss: 0.009725738098505592\n",
      "Training loss: 0.00972568004312326\n",
      "Training loss: 0.0097256243770108\n",
      "Training loss: 0.009725564177209527\n",
      "Training loss: 0.009725506426155736\n",
      "Training loss: 0.00972545029950527\n",
      "Training loss: 0.009725390189704803\n",
      "Training loss: 0.00972533304795913\n",
      "Training loss: 0.009725275176226025\n",
      "Training loss: 0.009725214279944766\n",
      "Training loss: 0.009725159727164506\n",
      "Training loss: 0.00972509780091332\n",
      "Training loss: 0.00972503811658689\n",
      "Training loss: 0.009724983024606532\n",
      "Training loss: 0.0097249214017153\n",
      "Training loss: 0.009724861433755033\n",
      "Training loss: 0.009724804596808351\n",
      "Training loss: 0.009724741336428106\n",
      "Training loss: 0.009724684620671524\n",
      "Training loss: 0.009724623729847802\n",
      "Training loss: 0.009724561070008818\n",
      "Training loss: 0.00972450405731262\n",
      "Training loss: 0.00972444289564672\n",
      "Training loss: 0.009724380157645901\n",
      "Training loss: 0.009724320940262954\n",
      "Training loss: 0.009724258084582466\n",
      "Training loss: 0.009724199158330418\n",
      "Training loss: 0.009724135990977437\n",
      "Training loss: 0.009724073884216044\n",
      "Training loss: 0.009724014535088496\n",
      "Training loss: 0.009723950388038501\n",
      "Training loss: 0.009723888410980085\n",
      "Training loss: 0.009723827690155689\n",
      "Training loss: 0.009723763917804091\n",
      "Training loss: 0.009723701032095628\n",
      "Training loss: 0.009723639838290164\n",
      "Training loss: 0.00972357402340812\n",
      "Training loss: 0.00972351541608772\n",
      "Training loss: 0.009723448506287712\n",
      "Training loss: 0.009723384409533594\n",
      "Training loss: 0.009723324967598389\n",
      "Training loss: 0.009723258856190848\n",
      "Training loss: 0.00972319467406731\n",
      "Training loss: 0.009723133089012565\n",
      "Training loss: 0.009723067376754869\n",
      "Training loss: 0.009723002401363541\n",
      "Training loss: 0.009722942040018279\n",
      "Training loss: 0.009722875292853213\n",
      "Training loss: 0.009722810019348377\n",
      "Training loss: 0.00972274643782116\n",
      "Training loss: 0.009722679034906843\n",
      "Training loss: 0.009722616885347618\n",
      "Training loss: 0.009722549125387097\n",
      "Training loss: 0.00972248207570068\n",
      "Training loss: 0.009722419338147463\n",
      "Training loss: 0.009722351606851047\n",
      "Training loss: 0.009722285757741422\n",
      "Training loss: 0.009722220201493813\n",
      "Training loss: 0.009722150234884928\n",
      "Training loss: 0.009722087949155463\n",
      "Training loss: 0.00972201724330948\n",
      "Training loss: 0.009721949628856617\n",
      "Training loss: 0.009721887167895495\n",
      "Training loss: 0.009721816202490947\n",
      "Training loss: 0.009721748976958024\n",
      "Training loss: 0.009721683443546542\n",
      "Training loss: 0.009721613079828608\n",
      "Training loss: 0.009721543266487035\n",
      "Training loss: 0.009721478386980474\n",
      "Training loss: 0.009721409271866046\n",
      "Training loss: 0.009721338763512406\n",
      "Training loss: 0.009721271831797388\n",
      "Training loss: 0.009721200006892304\n",
      "Training loss: 0.009721134495724357\n",
      "Training loss: 0.009721062913122655\n",
      "Training loss: 0.009720992704002071\n",
      "Training loss: 0.00972092541588961\n",
      "Training loss: 0.009720853772314826\n",
      "Training loss: 0.009720781201925854\n",
      "Training loss: 0.009720715734485847\n",
      "Training loss: 0.009720642005675715\n",
      "Training loss: 0.009720569482750234\n",
      "Training loss: 0.009720503754905824\n",
      "Training loss: 0.009720428969061726\n",
      "Training loss: 0.009720357310907259\n",
      "Training loss: 0.009720288355001414\n",
      "Training loss: 0.009720214822500402\n",
      "Training loss: 0.009720141167247565\n",
      "Training loss: 0.00972007256609187\n",
      "Training loss: 0.009719997343758752\n",
      "Training loss: 0.009719927861879007\n",
      "Training loss: 0.009719852444523938\n",
      "Training loss: 0.00971977955333172\n",
      "Training loss: 0.009719707917837744\n",
      "Training loss: 0.009719631600919444\n",
      "Training loss: 0.009719557093921145\n",
      "Training loss: 0.00971948611633502\n",
      "Training loss: 0.009719409354736615\n",
      "Training loss: 0.009719333857631378\n",
      "Training loss: 0.009719261178658721\n",
      "Training loss: 0.009719182768457135\n",
      "Training loss: 0.009719110511317058\n",
      "Training loss: 0.00971903246426455\n",
      "Training loss: 0.009718955488045525\n",
      "Training loss: 0.009718877052989494\n",
      "Training loss: 0.009718802857340654\n",
      "Training loss: 0.009718724854230382\n",
      "Training loss: 0.009718643999279904\n",
      "Training loss: 0.009718569935620775\n",
      "Training loss: 0.009718488196413607\n",
      "Training loss: 0.009718413153801488\n",
      "Training loss: 0.009718332371341\n",
      "Training loss: 0.0097182539046513\n",
      "Training loss: 0.009718175886068343\n",
      "Training loss: 0.009718093378814439\n",
      "Training loss: 0.009718013616625245\n",
      "Training loss: 0.009717936445475575\n",
      "Training loss: 0.009717853650570383\n",
      "Training loss: 0.009717771934248616\n",
      "Training loss: 0.009717692727722337\n",
      "Training loss: 0.009717607569745811\n",
      "Training loss: 0.00971753072443167\n",
      "Training loss: 0.009717443891243748\n",
      "Training loss: 0.009717365429388563\n",
      "Training loss: 0.009717278382437166\n",
      "Training loss: 0.00971720041752936\n",
      "Training loss: 0.009717114172584897\n",
      "Training loss: 0.009717029954862623\n",
      "Training loss: 0.009716950927583564\n",
      "Training loss: 0.009716863250051389\n",
      "Training loss: 0.00971677688200832\n",
      "Training loss: 0.00971669759574645\n",
      "Training loss: 0.009716607344820972\n",
      "Training loss: 0.009716526530589383\n",
      "Training loss: 0.009716437361738085\n",
      "Training loss: 0.009716355303419747\n",
      "Training loss: 0.009716266177326478\n",
      "Training loss: 0.00971618062785897\n",
      "Training loss: 0.009716094423795028\n",
      "Training loss: 0.009716003233836102\n",
      "Training loss: 0.009715919727670264\n",
      "Training loss: 0.009715826449803566\n",
      "Training loss: 0.009715742143980877\n",
      "Training loss: 0.00971564985561871\n",
      "Training loss: 0.009715564556378448\n",
      "Training loss: 0.009715471515545407\n",
      "Training loss: 0.009715381788829722\n",
      "Training loss: 0.009715292835792913\n",
      "Training loss: 0.009715198092194424\n",
      "Training loss: 0.009715111597182406\n",
      "Training loss: 0.009715017243561823\n",
      "Training loss: 0.009714923609828369\n",
      "Training loss: 0.009714834621917575\n",
      "Training loss: 0.009714739203897356\n",
      "Training loss: 0.009714649060581674\n",
      "Training loss: 0.009714553823958121\n",
      "Training loss: 0.009714461203588562\n",
      "Training loss: 0.009714368963652154\n",
      "Training loss: 0.009714270910239414\n",
      "Training loss: 0.009714182951800588\n",
      "Training loss: 0.009714083352879349\n",
      "Training loss: 0.009713989145881227\n",
      "Training loss: 0.009713897662861232\n",
      "Training loss: 0.009713798913502212\n",
      "Training loss: 0.009713700487534031\n",
      "Training loss: 0.009713608910769824\n",
      "Training loss: 0.009713506582544336\n",
      "Training loss: 0.009713411815598608\n",
      "Training loss: 0.009713310772330194\n",
      "Training loss: 0.009713210365621146\n",
      "Training loss: 0.009713106615410297\n",
      "Training loss: 0.009713011709487745\n",
      "Training loss: 0.009712905593120337\n",
      "Training loss: 0.00971280526609158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.009712701846884842\n",
      "Training loss: 0.009712602155409998\n",
      "Training loss: 0.009712496380876352\n",
      "Training loss: 0.009712393326229996\n",
      "Training loss: 0.009712292718820072\n",
      "Training loss: 0.009712185268707606\n",
      "Training loss: 0.009712082344237856\n",
      "Training loss: 0.009711973524711353\n",
      "Training loss: 0.00971187320262574\n",
      "Training loss: 0.009711763050428148\n",
      "Training loss: 0.009711658118533688\n",
      "Training loss: 0.009711545911503605\n",
      "Training loss: 0.009711442861610005\n",
      "Training loss: 0.009711331990361538\n",
      "Training loss: 0.009711226215018832\n",
      "Training loss: 0.009711114482906019\n",
      "Training loss: 0.009711002660872213\n",
      "Training loss: 0.009710899969193347\n",
      "Training loss: 0.009710785475437892\n",
      "Training loss: 0.009710676611625644\n",
      "Training loss: 0.009710565240991302\n",
      "Training loss: 0.009710449868865687\n",
      "Training loss: 0.009710342436685638\n",
      "Training loss: 0.009710227143785043\n",
      "Training loss: 0.009710114620087832\n",
      "Training loss: 0.009709996434714327\n",
      "Training loss: 0.009709889116772558\n",
      "Training loss: 0.009709768678466424\n",
      "Training loss: 0.009709656825047338\n",
      "Training loss: 0.009709539309513272\n",
      "Training loss: 0.009709426158678336\n",
      "Training loss: 0.009709307252063116\n",
      "Training loss: 0.009709192706912064\n",
      "Training loss: 0.009709070820658312\n",
      "Training loss: 0.009708958186427724\n",
      "Training loss: 0.009708837120741536\n",
      "Training loss: 0.009708715576793661\n",
      "Training loss: 0.009708602875006687\n",
      "Training loss: 0.009708478445903902\n",
      "Training loss: 0.009708360426851427\n",
      "Training loss: 0.009708237659143684\n",
      "Training loss: 0.009708119499554718\n",
      "Training loss: 0.009707994386272994\n",
      "Training loss: 0.00970787254054385\n",
      "Training loss: 0.009707751306087985\n",
      "Training loss: 0.009707625262315757\n",
      "Training loss: 0.009707499555182077\n",
      "Training loss: 0.009707377789091417\n",
      "Training loss: 0.009707248828161397\n",
      "Training loss: 0.00970712430838437\n",
      "Training loss: 0.009706999642787855\n",
      "Training loss: 0.009706869154779483\n",
      "Training loss: 0.009706744027074943\n",
      "Training loss: 0.009706612435002069\n",
      "Training loss: 0.00970648776340267\n",
      "Training loss: 0.00970635594275613\n",
      "Training loss: 0.009706224211798464\n",
      "Training loss: 0.009706092822633768\n",
      "Training loss: 0.009705965386302053\n",
      "Training loss: 0.009705829577764979\n",
      "Training loss: 0.00970569731693268\n",
      "Training loss: 0.009705566292677152\n",
      "Training loss: 0.009705428367010854\n",
      "Training loss: 0.00970529543615405\n",
      "Training loss: 0.009705155394963225\n",
      "Training loss: 0.009705025505635317\n",
      "Training loss: 0.009704884065620168\n",
      "Training loss: 0.00970474927173164\n",
      "Training loss: 0.009704606487104615\n",
      "Training loss: 0.009704471813357023\n",
      "Training loss: 0.009704327091813174\n",
      "Training loss: 0.009704191452629945\n",
      "Training loss: 0.009704042690551376\n",
      "Training loss: 0.00970390784520492\n",
      "Training loss: 0.009703760631218448\n",
      "Training loss: 0.009703619717644977\n",
      "Training loss: 0.009703472882288228\n",
      "Training loss: 0.009703334036380313\n",
      "Training loss: 0.009703185369518568\n",
      "Training loss: 0.009703044116227875\n",
      "Training loss: 0.00970289471871568\n",
      "Training loss: 0.00970274653888376\n",
      "Training loss: 0.00970260462281374\n",
      "Training loss: 0.009702451512345394\n",
      "Training loss: 0.009702302543853663\n",
      "Training loss: 0.009702156166649323\n",
      "Training loss: 0.009702001205316144\n",
      "Training loss: 0.009701852761930772\n",
      "Training loss: 0.00970169405876474\n",
      "Training loss: 0.009701546918153605\n",
      "Training loss: 0.009701390468345273\n",
      "Training loss: 0.009701236422502023\n",
      "Training loss: 0.009701078471961673\n",
      "Training loss: 0.009700922710939695\n",
      "Training loss: 0.009700768465519795\n",
      "Training loss: 0.009700606352271681\n",
      "Training loss: 0.009700452287714793\n",
      "Training loss: 0.009700285927004136\n",
      "Training loss: 0.009700132292907969\n",
      "Training loss: 0.009699967307923787\n",
      "Training loss: 0.009699808211976408\n",
      "Training loss: 0.00969964600144693\n",
      "Training loss: 0.009699480059363282\n",
      "Training loss: 0.009699320525656453\n",
      "Training loss: 0.009699152981222184\n",
      "Training loss: 0.00969898844838978\n",
      "Training loss: 0.009698819967330772\n",
      "Training loss: 0.009698651266765605\n",
      "Training loss: 0.009698487226317162\n",
      "Training loss: 0.009698314340386014\n",
      "Training loss: 0.009698148435042254\n",
      "Training loss: 0.009697971033016258\n",
      "Training loss: 0.009697806558123214\n",
      "Training loss: 0.009697631611269143\n",
      "Training loss: 0.009697459343150944\n",
      "Training loss: 0.009697283069882616\n",
      "Training loss: 0.009697107818138108\n",
      "Training loss: 0.00969693509718802\n",
      "Training loss: 0.009696755436681487\n",
      "Training loss: 0.009696580393010162\n",
      "Training loss: 0.009696396589324354\n",
      "Training loss: 0.00969622391586304\n",
      "Training loss: 0.009696040322103014\n",
      "Training loss: 0.009695860947976026\n",
      "Training loss: 0.009695675653361295\n",
      "Training loss: 0.009695494231253227\n",
      "Training loss: 0.009695308539343422\n",
      "Training loss: 0.009695127758749476\n",
      "Training loss: 0.009694939358522553\n",
      "Training loss: 0.009694750086441313\n",
      "Training loss: 0.009694567160885655\n",
      "Training loss: 0.009694374848473348\n",
      "Training loss: 0.009694185949547511\n",
      "Training loss: 0.009693991881527145\n",
      "Training loss: 0.009693806552153756\n",
      "Training loss: 0.009693609465748154\n",
      "Training loss: 0.009693418080155653\n",
      "Training loss: 0.00969321979428373\n",
      "Training loss: 0.009693029357717152\n",
      "Training loss: 0.009692827500564162\n",
      "Training loss: 0.00969263441306672\n",
      "Training loss: 0.009692431081752852\n",
      "Training loss: 0.009692231772423539\n",
      "Training loss: 0.009692031286991368\n",
      "Training loss: 0.009691831343606606\n",
      "Training loss: 0.00969162351659123\n",
      "Training loss: 0.009691424053070698\n",
      "Training loss: 0.009691211399978037\n",
      "Training loss: 0.009691010163538344\n",
      "Training loss: 0.009690800482052586\n",
      "Training loss: 0.009690591967644229\n",
      "Training loss: 0.009690375567613622\n",
      "Training loss: 0.009690173854081378\n",
      "Training loss: 0.009689953240061814\n",
      "Training loss: 0.00968974206368682\n",
      "Training loss: 0.009689526998926523\n",
      "Training loss: 0.009689306045953808\n",
      "Training loss: 0.009689093012759005\n",
      "Training loss: 0.009688873887031244\n",
      "Training loss: 0.009688650016500558\n",
      "Training loss: 0.009688433775767115\n",
      "Training loss: 0.009688206812975685\n",
      "Training loss: 0.009687985029404569\n",
      "Training loss: 0.009687755156151095\n",
      "Training loss: 0.009687536086511966\n",
      "Training loss: 0.009687302104228353\n",
      "Training loss: 0.009687075100109011\n",
      "Training loss: 0.009686840849316194\n",
      "Training loss: 0.009686613613528398\n",
      "Training loss: 0.009686376513517015\n",
      "Training loss: 0.009686146191364443\n",
      "Training loss: 0.009685907264163594\n",
      "Training loss: 0.009685675068885655\n",
      "Training loss: 0.009685434051586827\n",
      "Training loss: 0.00968519854089785\n",
      "Training loss: 0.009684953043839916\n",
      "Training loss: 0.009684712373953179\n",
      "Training loss: 0.009684469668921145\n",
      "Training loss: 0.009684226493997144\n",
      "Training loss: 0.00968397628739785\n",
      "Training loss: 0.00968373319532385\n",
      "Training loss: 0.009683479795076666\n",
      "Training loss: 0.009683224798371332\n",
      "Training loss: 0.009682980723445226\n",
      "Training loss: 0.00968272166991619\n",
      "Training loss: 0.009682466455856435\n",
      "Training loss: 0.009682208289239142\n",
      "Training loss: 0.009681944806441642\n",
      "Training loss: 0.009681689193654539\n",
      "Training loss: 0.009681422556816406\n",
      "Training loss: 0.009681160392312142\n",
      "Training loss: 0.009680890658942422\n",
      "Training loss: 0.009680627655782231\n",
      "Training loss: 0.009680353127568618\n",
      "Training loss: 0.009680087462101403\n",
      "Training loss: 0.00967981212666635\n",
      "Training loss: 0.009679537812200666\n",
      "Training loss: 0.009679259492989539\n",
      "Training loss: 0.009678983675021116\n",
      "Training loss: 0.009678701483004634\n",
      "Training loss: 0.009678417246246473\n",
      "Training loss: 0.009678133814826751\n",
      "Training loss: 0.009677852718495548\n",
      "Training loss: 0.009677558650722493\n",
      "Training loss: 0.009677270060000031\n",
      "Training loss: 0.0096769667982329\n",
      "Training loss: 0.009676678001892399\n",
      "Training loss: 0.009676372498367357\n",
      "Training loss: 0.009676074231507292\n",
      "Training loss: 0.009675766505035347\n",
      "Training loss: 0.009675465272860685\n",
      "Training loss: 0.009675151118992446\n",
      "Training loss: 0.009674842085143307\n",
      "Training loss: 0.009674526988717562\n",
      "Training loss: 0.009674215870058745\n",
      "Training loss: 0.00967389430987352\n",
      "Training loss: 0.00967357734777408\n",
      "Training loss: 0.009673246576738508\n",
      "Training loss: 0.009672930086804298\n",
      "Training loss: 0.00967259907655019\n",
      "Training loss: 0.009672268775506183\n",
      "Training loss: 0.009671931582677349\n",
      "Training loss: 0.009671601457788443\n",
      "Training loss: 0.009671258720360563\n",
      "Training loss: 0.009670922757254171\n",
      "Training loss: 0.009670572494712945\n",
      "Training loss: 0.009670227880947655\n",
      "Training loss: 0.009669879300750896\n",
      "Training loss: 0.00966952334875104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.009669167064060256\n",
      "Training loss: 0.009668812676211948\n",
      "Training loss: 0.009668446958230038\n",
      "Training loss: 0.009668089978465043\n",
      "Training loss: 0.009667723515967144\n",
      "Training loss: 0.009667354126478381\n",
      "Training loss: 0.009666987837756222\n",
      "Training loss: 0.009666615450994287\n",
      "Training loss: 0.009666244762946884\n",
      "Training loss: 0.009665861972940771\n",
      "Training loss: 0.009665493181174793\n",
      "Training loss: 0.009665102877277629\n",
      "Training loss: 0.009664726618948074\n",
      "Training loss: 0.009664336062570465\n",
      "Training loss: 0.009663948855484735\n",
      "Training loss: 0.009663559041280379\n",
      "Training loss: 0.009663159287608058\n",
      "Training loss: 0.009662769979439715\n",
      "Training loss: 0.009662362009035122\n",
      "Training loss: 0.009661965450846553\n",
      "Training loss: 0.00966155562267148\n",
      "Training loss: 0.009661151683515305\n",
      "Training loss: 0.009660733903880418\n",
      "Training loss: 0.009660327469133286\n",
      "Training loss: 0.009659901531569365\n",
      "Training loss: 0.00965948950466266\n",
      "Training loss: 0.009659059678743448\n",
      "Training loss: 0.00965863991826824\n",
      "Training loss: 0.009658205693809836\n",
      "Training loss: 0.009657769641430376\n",
      "Training loss: 0.009657340823761755\n",
      "Training loss: 0.009656893058101218\n",
      "Training loss: 0.009656458056350231\n",
      "Training loss: 0.00965600687062912\n",
      "Training loss: 0.009655562555743534\n",
      "Training loss: 0.009655105679350667\n",
      "Training loss: 0.009654650712906047\n",
      "Training loss: 0.009654195307963284\n",
      "Training loss: 0.009653724205486651\n",
      "Training loss: 0.00965326777347556\n",
      "Training loss: 0.009652792251936403\n",
      "Training loss: 0.009652316235049701\n",
      "Training loss: 0.009651845929390092\n",
      "Training loss: 0.009651356173183375\n",
      "Training loss: 0.009650878052307056\n",
      "Training loss: 0.009650383554809232\n",
      "Training loss: 0.009649896647192784\n",
      "Training loss: 0.009649393929896551\n",
      "Training loss: 0.009648892281826738\n",
      "Training loss: 0.009648392611232239\n",
      "Training loss: 0.009647880465328131\n",
      "Training loss: 0.009647366414342355\n",
      "Training loss: 0.009646852167980037\n",
      "Training loss: 0.009646329395150149\n",
      "Training loss: 0.009645801348398497\n",
      "Training loss: 0.009645277379000247\n",
      "Training loss: 0.009644737203149407\n",
      "Training loss: 0.009644205147243162\n",
      "Training loss: 0.009643652607476714\n",
      "Training loss: 0.009643115475715797\n",
      "Training loss: 0.009642556595506368\n",
      "Training loss: 0.00964200770268764\n",
      "Training loss: 0.00964144429377384\n",
      "Training loss: 0.00964087472741779\n",
      "Training loss: 0.009640316878669775\n",
      "Training loss: 0.009639738143003292\n",
      "Training loss: 0.009639156919435357\n",
      "Training loss: 0.00963857978978494\n",
      "Training loss: 0.009637992566838434\n",
      "Training loss: 0.00963739413468564\n",
      "Training loss: 0.009636800589943276\n",
      "Training loss: 0.009636206094040103\n",
      "Training loss: 0.009635587830280179\n",
      "Training loss: 0.009634989158409966\n",
      "Training loss: 0.009634374729909856\n",
      "Training loss: 0.009633740640953136\n",
      "Training loss: 0.009633128204252045\n",
      "Training loss: 0.009632492542076872\n",
      "Training loss: 0.009631855621650004\n",
      "Training loss: 0.009631217267851959\n",
      "Training loss: 0.009630558062508744\n",
      "Training loss: 0.009629908352161482\n",
      "Training loss: 0.009629246694038094\n",
      "Training loss: 0.009628573273173938\n",
      "Training loss: 0.009627903277413061\n",
      "Training loss: 0.009627212712440266\n",
      "Training loss: 0.00962652517561206\n",
      "Training loss: 0.009625830651945128\n",
      "Training loss: 0.009625127486205571\n",
      "Training loss: 0.009624413776441436\n",
      "Training loss: 0.009623690467850988\n",
      "Training loss: 0.009622965985805813\n",
      "Training loss: 0.009622230176513997\n",
      "Training loss: 0.009621488800299956\n",
      "Training loss: 0.009620743600329504\n",
      "Training loss: 0.009619987158346688\n",
      "Training loss: 0.00961922113156969\n",
      "Training loss: 0.009618441400593189\n",
      "Training loss: 0.009617663504050734\n",
      "Training loss: 0.009616866694202697\n",
      "Training loss: 0.009616061143150036\n",
      "Training loss: 0.009615252301003964\n",
      "Training loss: 0.009614433440300286\n",
      "Training loss: 0.00961360517239141\n",
      "Training loss: 0.009612776513054032\n",
      "Training loss: 0.009611931196617999\n",
      "Training loss: 0.009611084608174406\n",
      "Training loss: 0.009610224010931591\n",
      "Training loss: 0.00960935480481422\n",
      "Training loss: 0.009608486714168056\n",
      "Training loss: 0.009607599403852723\n",
      "Training loss: 0.009606708687958265\n",
      "Training loss: 0.009605815990423861\n",
      "Training loss: 0.009604904039227622\n",
      "Training loss: 0.009603965540275624\n",
      "Training loss: 0.009603023991591545\n",
      "Training loss: 0.009602078601683457\n",
      "Training loss: 0.009601117519313442\n",
      "Training loss: 0.009600152886711443\n",
      "Training loss: 0.009599160910052928\n",
      "Training loss: 0.00959818476209796\n",
      "Training loss: 0.00959717507372022\n",
      "Training loss: 0.009596171619465467\n",
      "Training loss: 0.00959515464078915\n",
      "Training loss: 0.009594120980122673\n",
      "Training loss: 0.009593094859083173\n",
      "Training loss: 0.009592043364626759\n",
      "Training loss: 0.009590995045372643\n",
      "Training loss: 0.00958993014979413\n",
      "Training loss: 0.009588852355865119\n",
      "Training loss: 0.009587770977964748\n",
      "Training loss: 0.009586676201216324\n",
      "Training loss: 0.00958558126664627\n",
      "Training loss: 0.009584453078710803\n",
      "Training loss: 0.00958334344312814\n",
      "Training loss: 0.00958222585051463\n",
      "Training loss: 0.009581103711013656\n",
      "Training loss: 0.009579965265730678\n",
      "Training loss: 0.009578820311304205\n",
      "Training loss: 0.009577666217771064\n",
      "Training loss: 0.009576486302144709\n",
      "Training loss: 0.009575319366763348\n",
      "Training loss: 0.009574110686531046\n",
      "Training loss: 0.009572918013226643\n",
      "Training loss: 0.009571688899464552\n",
      "Training loss: 0.009570461577740396\n",
      "Training loss: 0.009569233426907093\n",
      "Training loss: 0.009567960700991595\n",
      "Training loss: 0.009566684028030127\n",
      "Training loss: 0.009565407194100789\n",
      "Training loss: 0.009564105774631024\n",
      "Training loss: 0.009562795624664567\n",
      "Training loss: 0.009561469803232532\n",
      "Training loss: 0.009560116258211734\n",
      "Training loss: 0.009558768576081626\n",
      "Training loss: 0.009557387537613407\n",
      "Training loss: 0.009556027598322844\n",
      "Training loss: 0.009554621935434417\n",
      "Training loss: 0.00955322185603878\n",
      "Training loss: 0.009551793846000946\n",
      "Training loss: 0.009550373387915529\n",
      "Training loss: 0.009548902515611446\n",
      "Training loss: 0.009547450497401366\n",
      "Training loss: 0.009545954158489793\n",
      "Training loss: 0.009544459949268723\n",
      "Training loss: 0.009542928177509787\n",
      "Training loss: 0.009541410556436489\n",
      "Training loss: 0.009539843419560745\n",
      "Training loss: 0.00953827354675384\n",
      "Training loss: 0.009536676926724578\n",
      "Training loss: 0.00953508226402266\n",
      "Training loss: 0.009533443502512808\n",
      "Training loss: 0.009531802999686891\n",
      "Training loss: 0.009530133918117363\n",
      "Training loss: 0.00952842266825242\n",
      "Training loss: 0.009526683892656762\n",
      "Training loss: 0.009524957579914776\n",
      "Training loss: 0.009523169009971534\n",
      "Training loss: 0.00952138490620585\n",
      "Training loss: 0.009519577924667599\n",
      "Training loss: 0.009517732440666192\n",
      "Training loss: 0.009515872222695492\n",
      "Training loss: 0.009513988152968477\n",
      "Training loss: 0.009512099693777537\n",
      "Training loss: 0.009510168737945983\n",
      "Training loss: 0.009508206619826553\n",
      "Training loss: 0.009506253181358884\n",
      "Training loss: 0.009504235095727018\n",
      "Training loss: 0.009502223648801952\n",
      "Training loss: 0.009500164006376354\n",
      "Training loss: 0.009498070822241824\n",
      "Training loss: 0.009495966013462849\n",
      "Training loss: 0.009493832437326174\n",
      "Training loss: 0.009491644853316534\n",
      "Training loss: 0.009489482243030705\n",
      "Training loss: 0.009487251847245463\n",
      "Training loss: 0.009484999981796664\n",
      "Training loss: 0.009482721240889712\n",
      "Training loss: 0.009480410383159742\n",
      "Training loss: 0.00947808305123158\n",
      "Training loss: 0.00947570089828172\n",
      "Training loss: 0.009473324766817186\n",
      "Training loss: 0.009470900321659734\n",
      "Training loss: 0.009468438927584024\n",
      "Training loss: 0.00946594077403721\n",
      "Training loss: 0.00946344314746581\n",
      "Training loss: 0.009460867569230012\n",
      "Training loss: 0.009458290419760417\n",
      "Training loss: 0.009455663765396829\n",
      "Training loss: 0.009453016885778284\n",
      "Training loss: 0.009450302813232257\n",
      "Training loss: 0.009447602089427814\n",
      "Training loss: 0.009444825828045832\n",
      "Training loss: 0.009442010675853802\n",
      "Training loss: 0.00943918442421506\n",
      "Training loss: 0.009436287044098173\n",
      "Training loss: 0.00943334001567697\n",
      "Training loss: 0.009430304322615954\n",
      "Training loss: 0.009427276456330787\n",
      "Training loss: 0.009424156371584082\n",
      "Training loss: 0.009421014230357763\n",
      "Training loss: 0.009417805052674403\n",
      "Training loss: 0.009414582722425723\n",
      "Training loss: 0.00941127514042774\n",
      "Training loss: 0.009407947794597854\n",
      "Training loss: 0.009404564322910017\n",
      "Training loss: 0.009401146699423267\n",
      "Training loss: 0.009397646699536978\n",
      "Training loss: 0.00939411898548061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.009390521601044479\n",
      "Training loss: 0.009386897183278564\n",
      "Training loss: 0.009383207688559556\n",
      "Training loss: 0.009379455434701724\n",
      "Training loss: 0.009375654119657661\n",
      "Training loss: 0.009371786911236786\n",
      "Training loss: 0.009367892636616785\n",
      "Training loss: 0.009363905593175058\n",
      "Training loss: 0.009359864072323533\n",
      "Training loss: 0.00935577154900664\n",
      "Training loss: 0.009351612377992315\n",
      "Training loss: 0.009347401880012587\n",
      "Training loss: 0.009343121718160197\n",
      "Training loss: 0.009338773584634034\n",
      "Training loss: 0.009334358233502631\n",
      "Training loss: 0.009329868974544514\n",
      "Training loss: 0.009325311026980861\n",
      "Training loss: 0.009320702658664502\n",
      "Training loss: 0.009315992950249425\n",
      "Training loss: 0.009311230571420092\n",
      "Training loss: 0.009306306153509756\n",
      "Training loss: 0.009301467447642764\n",
      "Training loss: 0.009296430427954293\n",
      "Training loss: 0.009291418269762033\n",
      "Training loss: 0.00928618579826498\n",
      "Training loss: 0.009281019279775106\n",
      "Training loss: 0.009275695932039685\n",
      "Training loss: 0.009270188801582641\n",
      "Training loss: 0.009264742052273619\n",
      "Training loss: 0.00925908422775234\n",
      "Training loss: 0.009253481791827712\n",
      "Training loss: 0.00924761612027019\n",
      "Training loss: 0.009241825783143181\n",
      "Training loss: 0.009235733604661388\n",
      "Training loss: 0.009229766667883096\n",
      "Training loss: 0.009223485935149255\n",
      "Training loss: 0.009217278020471282\n",
      "Training loss: 0.00921084378937603\n",
      "Training loss: 0.009204283152929421\n",
      "Training loss: 0.009197726864854065\n",
      "Training loss: 0.009190929271327157\n",
      "Training loss: 0.009184132043680517\n",
      "Training loss: 0.009177130026169851\n",
      "Training loss: 0.009170064576864277\n",
      "Training loss: 0.009163010043728475\n",
      "Training loss: 0.009155623641798272\n",
      "Training loss: 0.009148328464130467\n",
      "Training loss: 0.009140670522135944\n",
      "Training loss: 0.009133218119611238\n",
      "Training loss: 0.009125209608875908\n",
      "Training loss: 0.009117492938611863\n",
      "Training loss: 0.009109340112680707\n",
      "Training loss: 0.009101265307067237\n",
      "Training loss: 0.009092855588130784\n",
      "Training loss: 0.009084467132150966\n",
      "Training loss: 0.009075792489444798\n",
      "Training loss: 0.009067078734902544\n",
      "Training loss: 0.009058330154815988\n",
      "Training loss: 0.009049254036169178\n",
      "Training loss: 0.009040097717011397\n",
      "Training loss: 0.009030667339230237\n",
      "Training loss: 0.009021327603982903\n",
      "Training loss: 0.009011661988222522\n",
      "Training loss: 0.009001770749936265\n",
      "Training loss: 0.008991839653678755\n",
      "Training loss: 0.008981514746354032\n",
      "Training loss: 0.008971235507180037\n",
      "Training loss: 0.008960726804725958\n",
      "Training loss: 0.008949754269499706\n",
      "Training loss: 0.008939133063606183\n",
      "Training loss: 0.008927842083826543\n",
      "Training loss: 0.008916628770964235\n",
      "Training loss: 0.008905095566581683\n",
      "Training loss: 0.00889340208332717\n",
      "Training loss: 0.008881498793049843\n",
      "Training loss: 0.008869518868393643\n",
      "Training loss: 0.008857200511712326\n",
      "Training loss: 0.008844588022538107\n",
      "Training loss: 0.008832033897406956\n",
      "Training loss: 0.008819058829718814\n",
      "Training loss: 0.008806050953902892\n",
      "Training loss: 0.008793028613830703\n",
      "Training loss: 0.008779672318951668\n",
      "Training loss: 0.008766044470306733\n"
     ]
    }
   ],
   "source": [
    "n_input = 287\n",
    "\n",
    "model = nn.Sequential(nn.Linear(n_input, 512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(512, 256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(256, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 16),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(16, 8),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(8, 1),\n",
    "                      nn.ReLU())\n",
    "# Define the loss\n",
    "criterion = nn.MSELoss()\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "epochs = 1000\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for data, labels in train_loader:\n",
    "#         print(data.shape)\n",
    "        data = data.reshape(n_input,)\n",
    "        \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(labels)\n",
    "#         print(loss.item())\n",
    "#         print(loss)\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01487361]\n",
      " [0.01977256]\n",
      " [0.02386915]\n",
      " [0.03869469]\n",
      " [0.04628681]\n",
      " [0.0168243 ]\n",
      " [0.01419752]\n",
      " [0.02420133]\n",
      " [0.02415469]\n",
      " [0.02431733]\n",
      " [0.01408175]\n",
      " [0.01800091]\n",
      " [0.02109153]\n",
      " [0.0630459 ]\n",
      " [0.02124502]\n",
      " [0.03265345]\n",
      " [0.0369392 ]\n",
      " [0.01691914]\n",
      " [0.01863002]\n",
      " [0.02037854]\n",
      " [0.01350024]\n",
      " [0.03195705]\n",
      " [0.02221208]\n",
      " [0.01912282]\n",
      " [0.02835802]\n",
      " [0.01523714]\n",
      " [0.02178399]\n",
      " [0.01629552]\n",
      " [0.01983758]\n",
      " [0.0204764 ]\n",
      " [0.0329297 ]\n",
      " [0.04727734]\n",
      " [0.01298501]\n",
      " [0.01208562]\n",
      " [0.01257144]\n",
      " [0.0352212 ]\n",
      " [0.08402866]\n",
      " [0.01564727]\n",
      " [0.02401432]\n",
      " [0.05974607]\n",
      " [0.01896749]\n",
      " [0.01564269]\n",
      " [0.03299399]\n",
      " [0.02309116]\n",
      " [0.01503069]\n",
      " [0.01252675]\n",
      " [0.02195788]\n",
      " [0.03050249]\n",
      " [0.01721708]\n",
      " [0.02633187]\n",
      " [0.01325379]\n",
      " [0.03095495]\n",
      " [0.02999999]\n",
      " [0.01466636]\n",
      " [0.0311795 ]\n",
      " [0.01681894]\n",
      " [0.02281575]\n",
      " [0.01890623]\n",
      " [0.02240198]\n",
      " [0.01260576]\n",
      " [0.03667091]\n",
      " [0.01974936]\n",
      " [0.01763544]\n",
      " [0.02193528]\n",
      " [0.02803263]\n",
      " [0.01434928]\n",
      " [0.02726138]\n",
      " [0.04016424]\n",
      " [0.01965771]\n",
      " [0.02169222]\n",
      " [0.01517744]\n",
      " [0.02002875]\n",
      " [0.02346758]\n",
      " [0.01890782]\n",
      " [0.02595851]\n",
      " [0.02171763]\n",
      " [0.0173394 ]\n",
      " [0.01366593]\n",
      " [0.01965275]\n",
      " [0.0205442 ]\n",
      " [0.01379386]\n",
      " [0.01774537]\n",
      " [0.06388733]\n",
      " [0.02087006]\n",
      " [0.01712853]\n",
      " [0.02093669]\n",
      " [0.01784717]\n",
      " [0.02539318]\n",
      " [0.01880926]\n",
      " [0.03193139]\n",
      " [0.02776651]\n",
      " [0.02765846]\n",
      " [0.0268685 ]\n",
      " [0.01927332]\n",
      " [0.05154682]\n",
      " [0.04398772]\n",
      " [0.0155019 ]\n",
      " [0.02068718]\n",
      " [0.04415724]\n",
      " [0.01951224]\n",
      " [0.04094118]\n",
      " [0.02450603]\n",
      " [0.02830034]\n",
      " [0.01337273]\n",
      " [0.02399438]\n",
      " [0.02082098]\n",
      " [0.0215607 ]\n",
      " [0.01746971]\n",
      " [0.01815318]\n",
      " [0.03071498]\n",
      " [0.02198352]\n",
      " [0.02163805]\n",
      " [0.0217849 ]\n",
      " [0.02623334]\n",
      " [0.0347706 ]\n",
      " [0.02077886]\n",
      " [0.02488411]\n",
      " [0.02741426]\n",
      " [0.02219091]\n",
      " [0.03005738]\n",
      " [0.02464598]\n",
      " [0.04455491]\n",
      " [0.01552412]\n",
      " [0.02408528]\n",
      " [0.0193261 ]\n",
      " [0.02187557]\n",
      " [0.03031914]\n",
      " [0.01578283]\n",
      " [0.02070602]\n",
      " [0.0163539 ]\n",
      " [0.03383705]\n",
      " [0.0521474 ]\n",
      " [0.03964866]\n",
      " [0.04829773]\n",
      " [0.01676966]\n",
      " [0.03832538]\n",
      " [0.02435103]\n",
      " [0.03986422]\n",
      " [0.01916965]\n",
      " [0.01627397]\n",
      " [0.04977004]\n",
      " [0.02452048]\n",
      " [0.02408006]\n",
      " [0.0284674 ]\n",
      " [0.04325603]\n",
      " [0.02350566]\n",
      " [0.02565549]\n",
      " [0.02067894]\n",
      " [0.01658314]\n",
      " [0.0298094 ]\n",
      " [0.02140616]\n",
      " [0.026029  ]\n",
      " [0.02576375]\n",
      " [0.02876179]\n",
      " [0.0213144 ]\n",
      " [0.02939115]\n",
      " [0.01565509]\n",
      " [0.06558181]\n",
      " [0.02352247]\n",
      " [0.02389183]\n",
      " [0.02126044]\n",
      " [0.05274755]\n",
      " [0.01867909]\n",
      " [0.0653163 ]\n",
      " [0.0641083 ]\n",
      " [0.02407669]\n",
      " [0.01616935]\n",
      " [0.04250172]\n",
      " [0.05182723]\n",
      " [0.038505  ]\n",
      " [0.01802789]\n",
      " [0.02252127]\n",
      " [0.03777422]\n",
      " [0.02367447]\n",
      " [0.02616047]\n",
      " [0.051621  ]\n",
      " [0.01639757]\n",
      " [0.05772776]\n",
      " [0.02042949]\n",
      " [0.02198132]\n",
      " [0.02218158]\n",
      " [0.0375556 ]\n",
      " [0.02423157]\n",
      " [0.02751132]\n",
      " [0.01653623]\n",
      " [0.02485008]\n",
      " [0.02191109]\n",
      " [0.02259287]\n",
      " [0.02706236]\n",
      " [0.03296166]\n",
      " [0.02532339]\n",
      " [0.02445058]\n",
      " [0.02766712]\n",
      " [0.03727208]\n",
      " [0.02281482]\n",
      " [0.02456319]\n",
      " [0.05016967]\n",
      " [0.0765612 ]\n",
      " [0.02948754]\n",
      " [0.02926359]\n",
      " [0.01998525]\n",
      " [0.02012258]\n",
      " [0.02864554]\n",
      " [0.02067943]\n",
      " [0.02836254]\n",
      " [0.04537756]\n",
      " [0.02350479]\n",
      " [0.03959664]\n",
      " [0.03395063]\n",
      " [0.0469517 ]\n",
      " [0.02189879]\n",
      " [0.04873202]\n",
      " [0.02802917]\n",
      " [0.0623024 ]\n",
      " [0.02844185]\n",
      " [0.04788636]\n",
      " [0.02465857]\n",
      " [0.02308405]\n",
      " [0.03227064]\n",
      " [0.01638378]\n",
      " [0.03969311]\n",
      " [0.02133299]\n",
      " [0.02416391]\n",
      " [0.01834524]\n",
      " [0.0224587 ]\n",
      " [0.03969415]\n",
      " [0.04976667]\n",
      " [0.02781522]\n",
      " [0.05515903]\n",
      " [0.02150116]\n",
      " [0.04546773]\n",
      " [0.02376322]\n",
      " [0.04223811]\n",
      " [0.04657584]\n",
      " [0.03532098]\n",
      " [0.02837121]\n",
      " [0.04693593]\n",
      " [0.02781588]\n",
      " [0.01552789]\n",
      " [0.01616876]\n",
      " [0.02068172]\n",
      " [0.03251555]\n",
      " [0.01650301]\n",
      " [0.02420964]\n",
      " [0.02115142]\n",
      " [0.03146759]\n",
      " [0.02045379]\n",
      " [0.02911847]\n",
      " [0.02363177]\n",
      " [0.03008513]\n",
      " [0.02405493]\n",
      " [0.03184137]\n",
      " [0.02320712]\n",
      " [0.02660802]\n",
      " [0.01506879]\n",
      " [0.04020111]\n",
      " [0.0164289 ]\n",
      " [0.04700571]\n",
      " [0.04374596]\n",
      " [0.02331667]\n",
      " [0.02226342]\n",
      " [0.0282054 ]\n",
      " [0.02323649]\n",
      " [0.03230155]\n",
      " [0.03170294]\n",
      " [0.04279479]\n",
      " [0.0408239 ]\n",
      " [0.02130953]\n",
      " [0.02472719]\n",
      " [0.06243209]\n",
      " [0.02806285]\n",
      " [0.02129386]\n",
      " [0.02025633]\n",
      " [0.01625077]\n",
      " [0.01837702]\n",
      " [0.06837635]\n",
      " [0.02380423]\n",
      " [0.01842894]\n",
      " [0.02900957]\n",
      " [0.0195374 ]\n",
      " [0.05385099]\n",
      " [0.02699688]\n",
      " [0.03182856]\n",
      " [0.0496535 ]\n",
      " [0.02356372]\n",
      " [0.01882525]\n",
      " [0.0170133 ]\n",
      " [0.01666361]\n",
      " [0.02058919]\n",
      " [0.01707058]\n",
      " [0.02030996]\n",
      " [0.05244063]\n",
      " [0.04266342]\n",
      " [0.01973635]\n",
      " [0.03715216]\n",
      " [0.04662209]\n",
      " [0.046566  ]\n",
      " [0.02025274]\n",
      " [0.03048993]\n",
      " [0.02081482]\n",
      " [0.02836634]\n",
      " [0.02590862]\n",
      " [0.02417095]\n",
      " [0.02887705]\n",
      " [0.01662723]\n",
      " [0.04633549]\n",
      " [0.03841288]\n",
      " [0.06278138]\n",
      " [0.04340361]\n",
      " [0.03218681]\n",
      " [0.01856837]\n",
      " [0.02465422]\n",
      " [0.04588506]\n",
      " [0.03966656]\n",
      " [0.02448194]\n",
      " [0.03670856]\n",
      " [0.02173892]\n",
      " [0.01610626]\n",
      " [0.06501211]\n",
      " [0.01894016]\n",
      " [0.03494836]\n",
      " [0.0211889 ]\n",
      " [0.02941131]\n",
      " [0.03041599]\n",
      " [0.02349138]\n",
      " [0.02212754]\n",
      " [0.03347376]\n",
      " [0.02395866]\n",
      " [0.02186375]\n",
      " [0.0288685 ]\n",
      " [0.01877186]\n",
      " [0.03295086]\n",
      " [0.04920434]\n",
      " [0.02095911]\n",
      " [0.03590417]\n",
      " [0.01798196]\n",
      " [0.02559255]\n",
      " [0.01736265]\n",
      " [0.02528094]\n",
      " [0.02717628]\n",
      " [0.02104838]\n",
      " [0.02334173]\n",
      " [0.02709582]\n",
      " [0.01537567]\n",
      " [0.03583497]\n",
      " [0.05217662]\n",
      " [0.02048396]\n",
      " [0.05245229]\n",
      " [0.01559723]\n",
      " [0.03109486]\n",
      " [0.04586492]\n",
      " [0.02994397]\n",
      " [0.03317935]\n",
      " [0.02741469]\n",
      " [0.03322887]\n",
      " [0.0291448 ]\n",
      " [0.03414589]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    predicted = model(x.float()).data.numpy()\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01740786]\n",
      " [0.03032091]\n",
      " [0.01594885]\n",
      " [0.03014366]\n",
      " [0.05331788]\n",
      " [0.01626626]\n",
      " [0.03207823]\n",
      " [0.01983525]\n",
      " [0.04217457]\n",
      " [0.03751401]\n",
      " [0.0275722 ]\n",
      " [0.01771516]\n",
      " [0.01594556]\n",
      " [0.01457845]\n",
      " [0.01719477]\n",
      " [0.01691267]\n",
      " [0.04648752]\n",
      " [0.02303597]\n",
      " [0.02085268]\n",
      " [0.037814  ]\n",
      " [0.01679219]\n",
      " [0.01663066]\n",
      " [0.02156051]\n",
      " [0.01937419]\n",
      " [0.0186654 ]\n",
      " [0.01894936]\n",
      " [0.01331186]\n",
      " [0.01542701]\n",
      " [0.01691868]\n",
      " [0.02294255]\n",
      " [0.02535539]\n",
      " [0.01672294]\n",
      " [0.0136435 ]\n",
      " [0.0150495 ]\n",
      " [0.01490079]\n",
      " [0.02547451]\n",
      " [0.03765033]\n",
      " [0.05400706]\n",
      " [0.02033476]\n",
      " [0.01515894]\n",
      " [0.01457579]\n",
      " [0.01714767]\n",
      " [0.0393141 ]\n",
      " [0.02072759]\n",
      " [0.01440368]\n",
      " [0.01284483]\n",
      " [0.02475997]\n",
      " [0.02028128]\n",
      " [0.01520062]\n",
      " [0.02235178]\n",
      " [0.03993414]\n",
      " [0.0153775 ]\n",
      " [0.01514895]\n",
      " [0.02370428]\n",
      " [0.02267293]\n",
      " [0.01418394]\n",
      " [0.01697118]\n",
      " [0.02170886]\n",
      " [0.01489286]\n",
      " [0.01658599]\n",
      " [0.016198  ]\n",
      " [0.02707622]\n",
      " [0.01940226]\n",
      " [0.02119032]\n",
      " [0.01794192]\n",
      " [0.01337121]\n",
      " [0.01619164]\n",
      " [0.0261727 ]\n",
      " [0.03384973]\n",
      " [0.01326247]\n",
      " [0.01790935]\n",
      " [0.01778467]\n",
      " [0.02150306]\n",
      " [0.03901513]\n",
      " [0.02272325]\n",
      " [0.0474471 ]\n",
      " [0.02813214]\n",
      " [0.01921756]\n",
      " [0.0250066 ]\n",
      " [0.02033086]\n",
      " [0.02998976]\n",
      " [0.01948089]\n",
      " [0.03120352]\n",
      " [0.01702616]\n",
      " [0.03975463]\n",
      " [0.01902179]\n",
      " [0.02337771]\n",
      " [0.0283298 ]\n",
      " [0.04917984]\n",
      " [0.01959388]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    predicted_test = model(x_sub.float()).data.numpy()\n",
    "    print(predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.min_, scaler.scale_ = y_scaler.min_[0], y_scaler.scale_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training set is 112.47352353273475\n"
     ]
    }
   ],
   "source": [
    "pred_scaled = scaler.inverse_transform(predicted)\n",
    "true_scaled = y.data.numpy()\n",
    "# true_scaled = scaler.inverse_transform(y_normalized.data.numpy().reshape(357,1))\n",
    "print(\"MSE on training set is\" , mean_squared_error(true_scaled,pred_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_scaled = scaler.inverse_transform(predicted_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame({'true': true_scaled.reshape(357), 'pred': pred_scaled.reshape(357)}, columns=['true', 'pred'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv('training_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pred_test_scaled).to_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
